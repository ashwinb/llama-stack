"""Generate llama_stack.apis.agents.openai_responses package from openai-python types.

This script consumes the typed models that ship with the OpenAI Python SDK and
transforms them into the Pydantic ``BaseModel`` / ``TypedDict`` definitions that
the llama-stack project expects.  The goal is to keep our local schema file in
sync with OpenAI's source of truth without having to manually copy/paste large
chunks of code every time the upstream API evolves.

The generator walks the dependency graph that starts at
``ResponseCreateParams`` (the request payload) and ``Response`` (the response
object) and recursively captures every referenced type.  Each definition is then
rewritten so that it:

* is assigned to a focused module within ``llama_stack.apis.agents.openai_responses``
* uses ``pydantic.BaseModel`` instead of OpenAI's internal ``BaseModel``
* prefixes names with ``OpenAI`` so they remain isolated from our own models

Running the script overwrites the ``openai_responses`` package found at
``llama_stack/apis/agents/openai_responses``.  Use ``--output`` to write the
generated package somewhere else (for example when capturing review snapshots).
"""

import argparse
import ast
from collections import OrderedDict
from dataclasses import dataclass
import importlib
import pathlib
import shutil
from typing import Iterable

import libcst as cst

ROOT = pathlib.Path(__file__).resolve().parents[1]
OUTPUT_DIR = ROOT / "llama_stack" / "apis" / "agents" / "openai_responses"
MODULE_HEADER = (
    "# NOTE: This file is auto-generated by scripts/generate_openai_responses.py\n"
    "# Do not edit by hand.\n"
    "#\n"
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n"
    "# All rights reserved.\n"
    "#\n"
    "# This source code is licensed under the terms described in the LICENSE file in\n"
    "# the root directory of this source tree.\n"
)

TARGET_PREFIX = "OpenAI"
ROOTS = {
    ("openai.types.responses.response_create_params", "ResponseCreateParams"),
    ("openai.types.responses.response", "Response"),
}
TYPING_NAMES = {
    "Annotated",
    "Any",
    "Callable",
    "Dict",
    "Iterable",
    "List",
    "Mapping",
    "Optional",
    "Sequence",
    "Tuple",
    "Union",
}
TYPING_EXT_NAMES = {
    "Literal",
    "NotRequired",
    "Required",
    "TypeAlias",
    "TypedDict",
}
GROUP_ORDER = [
    "errors",
    "messages",
    "tool_calls",
    "tools",
    "outputs",
    "usage",
    "objects",
    "inputs",
    "requests",
    "configs",
    "shared",
]
SKIP_NAMES = {
    "False",
    "True",
    "None",
    "Field",
    "BaseModel",
    "json_schema_type",
    "register_schema",
}
SKIP_NAMES.update(TYPING_NAMES)
SKIP_NAMES.update(TYPING_EXT_NAMES)


@dataclass(frozen=True)
class DefinitionKey:
    module: str
    name: str


@dataclass
class ModuleData:
    module: cst.Module
    imports: dict[str, tuple[str, str]]
    local_defs: dict[str, cst.CSTNode]
    py_defs: dict[str, ast.AST]


@dataclass
class Definition:
    key: DefinitionKey
    node: cst.CSTNode
    register: bool
    decorate: bool
    py_node: ast.AST | None
    references: set[str]


@dataclass
class RenderedDefinition:
    name: str
    code: str
    register: bool
    decorate: bool
    dependencies: set[str]
    typing_names: set[str]
    typing_ext_names: set[str]
    pydantic_names: set[str]


_MODULE_CACHE: dict[str, ModuleData] = {}


def resolve_relative(module_path: str, level: int, target: str | None) -> str:
    if level == 0:
        return target or ""
    parts = module_path.split(".")
    base = parts[: len(parts) - level]
    if target:
        base.extend(target.split("."))
    return ".".join(base)


def load_module_data(module_path: str) -> ModuleData:
    cached = _MODULE_CACHE.get(module_path)
    if cached:
        return cached

    module = importlib.import_module(module_path)
    file = pathlib.Path(module.__file__).resolve()
    source = file.read_text()
    cst_module = cst.parse_module(source)
    py_module = ast.parse(source)

    imports: dict[str, tuple[str, str]] = {}
    module_prefix = module_path
    for node in py_module.body:
        if isinstance(node, ast.ImportFrom):
            if node.module == "__future__":
                continue
            module_name = resolve_relative(module_prefix, node.level or 0, node.module)
            if not module_name.startswith("openai.types"):
                continue
            for alias in node.names:
                if alias.name == "*":
                    continue
                asname = alias.asname or alias.name.split(".")[-1]
                original = alias.name.split(".")[-1]
                imports[asname] = (module_name, original)

    local_defs: dict[str, cst.CSTNode] = {}
    py_defs: dict[str, ast.AST] = {}
    for stmt, py_stmt in zip(cst_module.body, py_module.body):
        if isinstance(stmt, cst.ClassDef) and isinstance(py_stmt, ast.ClassDef):
            local_defs[stmt.name.value] = stmt
            py_defs[stmt.name.value] = py_stmt
            continue
        if isinstance(stmt, cst.SimpleStatementLine):
            for expr in stmt.body:
                target_name: str | None = None
                if isinstance(expr, cst.AnnAssign):
                    target = expr.target
                    if isinstance(target, cst.Name):
                        target_name = target.value
                elif isinstance(expr, cst.Assign):
                    if len(expr.targets) == 1 and isinstance(expr.targets[0].target, cst.Name):
                        target_name = expr.targets[0].target.value
                if target_name:
                    local_defs[target_name] = expr
        if isinstance(py_stmt, (ast.AnnAssign, ast.Assign)):
            if isinstance(py_stmt, ast.Assign):
                if len(py_stmt.targets) != 1 or not isinstance(py_stmt.targets[0], ast.Name):
                    continue
                py_defs[py_stmt.targets[0].id] = py_stmt
            elif isinstance(py_stmt, ast.AnnAssign) and isinstance(py_stmt.target, ast.Name):
                py_defs[py_stmt.target.id] = py_stmt

    module_data = ModuleData(module=cst_module, imports=imports, local_defs=local_defs, py_defs=py_defs)
    _MODULE_CACHE[module_path] = module_data
    return module_data


class ReferenceCollector(cst.CSTVisitor):
    def __init__(self) -> None:
        self.names: set[str] = set()

    def visit_Name(self, node: cst.Name) -> None:  # noqa: D401 - libcst visitor API
        self.names.add(node.value)


def collect_references(node: cst.CSTNode) -> set[str]:
    collector = ReferenceCollector()
    node.visit(collector)
    return collector.names


def is_typed_dict(node: cst.ClassDef) -> bool:
    for base in node.bases:
        value = base.value
        if isinstance(value, cst.Name) and value.value == "TypedDict":
            return True
        if isinstance(value, cst.Attribute) and value.attr.value == "TypedDict":
            return True
    return False


def determine_group(definition: Definition, target_name: str) -> str:
    module_tail = definition.key.module.split(".")[-1]
    lower_module = module_tail.lower()
    lower_name = target_name.lower()

    if definition.key.module.startswith("openai.types.shared"):
        return "shared"
    if "error" in lower_module or "error" in lower_name:
        return "errors"
    if "usage" in lower_module or "usage" in lower_name:
        return "usage"
    if "tool_call" in lower_module:
        return "tool_calls"
    if "tool_choice" in lower_module:
        return "tools"
    if "web_search" in lower_module and "tool" not in lower_module:
        return "tool_calls"
    if "tool_param" in lower_module or lower_module.endswith("tool") or "tool_" in lower_module:
        return "tools"
    if "message" in lower_module or "annotation" in lower_module:
        return "messages"
    if "create_params" in lower_module:
        return "requests"
    if "prompt" in lower_module:
        return "inputs"
    if "input" in lower_module:
        return "inputs"
    if "format" in lower_module or "config" in lower_module or "chat_model" in lower_module or "responses_model" in lower_module:
        return "configs"
    if "reasoning" in lower_module:
        return "outputs"
    if "output" in lower_module or "content" in lower_module:
        return "outputs"
    if "status" in lower_module or "conversation" in lower_module or "stream" in lower_module or "object" in lower_module:
        return "objects"
    return "objects"


def collect_code_names(code: str) -> set[str]:
    parsed = ast.parse(code)
    names: set[str] = set()

    class Visitor(ast.NodeVisitor):
        def visit_Name(self, node: ast.Name) -> None:  # noqa: D401
            names.add(node.id)

    Visitor().visit(parsed)
    names.discard("json_schema_type")
    names.discard("register_schema")
    return names


def format_relative_import(module: str, names: list[str]) -> str:
    if len(names) == 1:
        return f"from .{module} import {names[0]}"
    body = [f"from .{module} import ("]
    body.extend(f"    {name}," for name in names)
    body.append(")")
    return "\n".join(body)


def is_type_alias_assign(node: ast.Assign) -> bool:
    value = node.value
    return not isinstance(value, (ast.Constant, ast.List, ast.Dict, ast.Set))


def gather_definitions(roots: Iterable[DefinitionKey]) -> list[Definition]:
    processed: dict[DefinitionKey, Definition] = OrderedDict()
    visiting: set[DefinitionKey] = set()

    def visit(key: DefinitionKey) -> None:
        if key in processed:
            return
        if key in visiting:
            return
        visiting.add(key)

        module_data = load_module_data(key.module)
        node = module_data.local_defs.get(key.name)
        if not node:
            raise ValueError(f"Unable to locate definition for {key.name} in {key.module}")

        references = collect_references(node)
        reference_names = {name for name in references if name not in SKIP_NAMES and name != key.name}
        dependencies: list[DefinitionKey] = []
        for name in reference_names:
            if name in module_data.local_defs:
                dependencies.append(DefinitionKey(key.module, name))
                continue
            target = module_data.imports.get(name)
            if target:
                module_name, original = target
                dependencies.append(DefinitionKey(module_name, original))

        for dep in dependencies:
            visit(dep)

        register = False
        decorate = False
        py_node = module_data.py_defs.get(key.name)
        if isinstance(node, cst.ClassDef):
            decorate = not is_typed_dict(node)
            register = decorate
        elif isinstance(node, cst.AnnAssign):
            register = True
        elif isinstance(node, cst.Assign) and isinstance(py_node, ast.Assign):
            register = is_type_alias_assign(py_node)

        processed[key] = Definition(
            key=key,
            node=node,
            register=register,
            decorate=decorate,
            py_node=py_node,
            references=reference_names,
        )
        visiting.remove(key)

    for root in roots:
        visit(root)

    return list(processed.values())


def compute_target_name(name: str) -> str:
    if name.startswith(TARGET_PREFIX):
        return name
    return f"{TARGET_PREFIX}{name}"


class NameRewriteTransformer(cst.CSTTransformer):
    def __init__(self, name_map: dict[str, str]):
        self.name_map = name_map

    def leave_Name(self, original_node: cst.Name, updated_node: cst.Name) -> cst.BaseExpression:
        replacement = self.name_map.get(updated_node.value)
        if replacement:
            return updated_node.with_changes(value=replacement)
        return updated_node

    def leave_Attribute(self, original_node: cst.Attribute, updated_node: cst.Attribute) -> cst.BaseExpression:
        if isinstance(updated_node.value, cst.Name):
            replacement = self.name_map.get(updated_node.value.value)
            if replacement:
                return cst.Name(replacement)
        return updated_node


def _extract_string(stmt: cst.BaseStatement) -> str | None:
    if not isinstance(stmt, cst.SimpleStatementLine):
        return None
    if len(stmt.body) != 1:
        return None
    expr = stmt.body[0]
    if not isinstance(expr, cst.Expr):
        return None
    value = expr.value
    if not isinstance(value, cst.SimpleString):
        return None
    return ast.literal_eval(value.value)


def _format_param_doc(name: str, description: str) -> str:
    description = description.strip()
    if not description:
        return f"    :param {name}:"
    lines = description.splitlines()
    first = lines[0]
    rest = lines[1:]
    formatted = f"    :param {name}: {first}"
    for line in rest:
        if line:
            formatted += "\n        " + line
        else:
            formatted += "\n"
    return formatted


def _rewrite_docstrings(definition: Definition, class_def: cst.ClassDef) -> cst.ClassDef:
    statements = list(class_def.body.body)
    new_body: list[cst.BaseStatement] = []
    param_docs: list[tuple[str, str]] = []

    summary: str | None = None
    if statements:
        potential = _extract_string(statements[0])
        if potential is not None:
            summary = potential.strip()
            statements = statements[1:]
        elif isinstance(definition.py_node, ast.ClassDef):
            docstring = ast.get_docstring(definition.py_node)
            if docstring:
                summary = docstring.strip()

    i = 0
    while i < len(statements):
        stmt = statements[i]
        handled = False
        if isinstance(stmt, cst.SimpleStatementLine) and stmt.body:
            first = stmt.body[0]
            target_name: str | None = None
            if isinstance(first, cst.AnnAssign):
                target = first.target
                if isinstance(target, cst.Name):
                    target_name = target.value
            elif isinstance(first, cst.Assign):
                if len(first.targets) == 1 and isinstance(first.targets[0].target, cst.Name):
                    target_name = first.targets[0].target.value
            if target_name:
                doc: str | None = None
                if i + 1 < len(statements):
                    potential_doc = _extract_string(statements[i + 1])
                    if potential_doc is not None:
                        doc = potential_doc
                        i += 1
                if doc:
                    param_docs.append((target_name, doc))
                new_body.append(stmt)
                handled = True
        if not handled:
            new_body.append(stmt)
        i += 1

    doc_lines: list[str] = []
    has_summary = bool(summary)
    if has_summary:
        doc_lines.append(summary)
    if param_docs:
        if has_summary:
            doc_lines.append("")
        doc_lines.extend(_format_param_doc(name, text) for name, text in param_docs)

    if doc_lines:
        content = "\n".join(doc_lines)
        if not has_summary:
            content = "\n" + content
        if "\n" in content:
            content = content + "\n"
        content = content.replace('"""', '\\"\\"\\"')
        doc_node = cst.SimpleStatementLine(
            body=[cst.Expr(value=cst.SimpleString(value=f'"""{content}"""'))]
        )
        new_body.insert(0, doc_node)

    return class_def.with_changes(body=class_def.body.with_changes(body=new_body))


def render_definitions(definitions: Iterable[Definition]) -> tuple[dict[str, list[RenderedDefinition]], dict[str, str]]:
    defs = list(definitions)
    name_map: dict[str, str] = {
        "PropertyInfo": "Field",
        "FieldInfo": "Field",
        "SequenceNotStr": "Sequence",
    }
    for definition in defs:
        name_map.setdefault(definition.key.name, compute_target_name(definition.key.name))

    grouped: dict[str, list[RenderedDefinition]] = {}
    name_to_group: dict[str, str] = {}

    for definition in defs:
        node = definition.node
        transformer = NameRewriteTransformer(name_map)
        rewritten = node.visit(transformer)

        target_name = name_map[definition.key.name]
        if isinstance(rewritten, cst.ClassDef):
            rewritten = rewritten.with_changes(name=cst.Name(target_name))
            if definition.decorate:
                decorator = cst.Decorator(cst.Name("json_schema_type"))
                rewritten = rewritten.with_changes(decorators=[decorator, *rewritten.decorators])
            rewritten = _rewrite_docstrings(definition, rewritten)
        elif isinstance(rewritten, cst.AnnAssign):
            rewritten = rewritten.with_changes(target=cst.Name(target_name))
        elif isinstance(rewritten, cst.Assign):
            targets = list(rewritten.targets)
            first = targets[0]
            if isinstance(first.target, cst.Name):
                targets[0] = first.with_changes(target=cst.Name(target_name))
                rewritten = rewritten.with_changes(targets=targets)

        code = cst.Module([]).code_for_node(rewritten)
        dependencies = {
            name_map[name]
            for name in definition.references
            if name in name_map and name != definition.key.name
        }
        used_names = collect_code_names(code)
        typing_names = {name for name in used_names if name in TYPING_NAMES}
        typing_ext_names = {name for name in used_names if name in TYPING_EXT_NAMES}
        pydantic_names = {name for name in used_names if name in {"BaseModel", "Field"}}

        group = determine_group(definition, target_name)
        name_to_group[target_name] = group
        grouped.setdefault(group, []).append(
            RenderedDefinition(
                name=target_name,
                code=code,
                register=definition.register,
                decorate=definition.decorate,
                dependencies=dependencies,
                typing_names=typing_names,
                typing_ext_names=typing_ext_names,
                pydantic_names=pydantic_names,
            )
        )

    return grouped, name_to_group


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Generate llama_stack.apis.agents.openai_responses package from openai-python types.",
    )
    parser.add_argument(
        "--output",
        type=pathlib.Path,
        default=OUTPUT_DIR,
        help=(
            "Directory to write the generated schema package. Defaults to the checked-in "
            "openai_responses package."
        ),
    )
    return parser.parse_args()


def main() -> None:
    args = parse_args()
    roots = [DefinitionKey(module=m, name=n) for m, n in ROOTS]
    definitions = gather_definitions(roots)
    grouped, name_to_group = render_definitions(definitions)

    output_dir = args.output
    if output_dir.exists():
        shutil.rmtree(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    module_order = [group for group in GROUP_ORDER if group in grouped]
    remaining_groups = sorted(group for group in grouped if group not in GROUP_ORDER)
    module_order.extend(remaining_groups)

    for group in module_order:
        definitions_in_group = grouped[group]
        typing_names = sorted(set().union(*(defn.typing_names for defn in definitions_in_group)))
        typing_ext_names = sorted(set().union(*(defn.typing_ext_names for defn in definitions_in_group)))
        pydantic_names = sorted(set().union(*(defn.pydantic_names for defn in definitions_in_group)))
        needs_json_schema_type = any(defn.decorate for defn in definitions_in_group)
        needs_register_schema = any(defn.register for defn in definitions_in_group)

        import_lines: list[str] = []
        if typing_names:
            import_lines.append("from typing import " + ", ".join(typing_names))
        if pydantic_names:
            import_lines.append("from pydantic import " + ", ".join(pydantic_names))
        if typing_ext_names:
            import_lines.append("from typing_extensions import " + ", ".join(typing_ext_names))

        schema_utils: list[str] = []
        if needs_json_schema_type:
            schema_utils.append("json_schema_type")
        if needs_register_schema:
            schema_utils.append("register_schema")
        if schema_utils:
            import_lines.append("from llama_stack.schema_utils import " + ", ".join(schema_utils))

        external_deps: dict[str, set[str]] = {}
        for defn in definitions_in_group:
            for dep in defn.dependencies:
                dep_group = name_to_group.get(dep)
                if not dep_group or dep_group == group:
                    continue
                external_deps.setdefault(dep_group, set()).add(dep)

        for dep_group in sorted(external_deps):
            names = sorted(external_deps[dep_group])
            import_lines.append(format_relative_import(dep_group, names))

        sections: list[str] = []
        for defn in definitions_in_group:
            block = defn.code
            if defn.register:
                block += f"\n\nregister_schema({defn.name}, name=\"{defn.name}\")"
            sections.append(block)

        module_text = MODULE_HEADER
        if import_lines:
            module_text += "\n".join(import_lines) + "\n\n"
        else:
            module_text += "\n"
        module_text += "\n\n\n".join(sections) + "\n"
        (output_dir / f"{group}.py").write_text(module_text)

    init_lines = [
        '"""OpenAI Responses schema re-exports."""',
        '',
        '# This package mirrors the original openai_responses.py module but splits the',
        '# definitions into focused submodules. The generator at',
        '# scripts/generate_openai_responses.py targets this layout so the code can be',
        '# updated automatically in manageable chunks.',
        '',
    ]

    all_names: list[str] = []
    for group in module_order:
        definitions_in_group = grouped[group]
        names = [defn.name for defn in definitions_in_group]
        all_names.extend(names)
        init_lines.append(format_relative_import(group, names))
        init_lines.append('')

    init_lines.append('__all__ = [')
    for name in all_names:
        init_lines.append(f"    '{name}',")
    init_lines.append(']')
    init_lines.append('')

    (output_dir / '__init__.py').write_text('\n'.join(init_lines))


if __name__ == "__main__":
    main()
